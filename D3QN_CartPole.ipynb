{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohZIaztPpAPA"
      },
      "source": [
        "## TODO:\n",
        "- [ ] Prioritized memory sweeping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGCLALeELO95"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FDHHNp3dd1cy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np #install version 1.26.4\n",
        "import random\n",
        "import gym #install version 0.25.2\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeL3hOZmLF-N"
      },
      "source": [
        "## Experience replay buffer\n",
        "\n",
        "- First, each step of experience is potentially used in many weight updates, which allows for greater data efficiency.\n",
        "- Second, learning directly from consecutive samples is inefficient, owing to the strong correlations between the samples; randomizing the samples breaks these correla- tions and therefore reduces the variance of the updates.\n",
        "- Third, when learning on- policy the current parameters determine the next data sample that the parameters are trained on. For example, if the maximizing action is to move left then the train- ing samples will be dominated by samples from the left-hand side; if the maximiz- ing action then switches to the right then the training distribution will also switch. It is easy to see how unwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically.\n",
        "\n",
        "By using experience replay the behaviour distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations or divergence in the parameters. Note that when learning by experience replay, it is necessary to learn off-policy (because our current parameters are different to those used to generate the sam- ple), which motivates the choice of Q-learning.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iNk6PYTWeIrW"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1jWYa0DncS6"
      },
      "source": [
        "## Define the dueling architecture for DRLagent\n",
        "- $\\theta$ --> paraameter correspoding to intial/lower layers\n",
        "- $\\alpha$ --> parameters corresponding to stream 1 (later/higher layer)\n",
        "- $\\beta$ --> parameters corresponding to stream 2 (later/higher layer)\n",
        "- $\\theta^-$ --> paraameter of target network (wights that get updated for $y$ follow soft-update technique)\n",
        "\n",
        "Some times it is not necessary to know action values for each state [ref dueling network paper]\n",
        "\n",
        "### Value function\n",
        "$V(s) = E_{a\\in\\pi(s)}[Q(s,a)]$ --> Measures how good it is to be in a state $s$\n",
        "\n",
        "### state action value ($Q$) function \n",
        "$Q(s,a) = E[R_t|s_t = s, a_t a, \\pi]$ --> Measures how good it is to take action $a$ in state $s$\n",
        "\n",
        "$Q$ function further can be rewritten following Bellman equation\n",
        "\n",
        "$Q(s,a) = E_{s'}[r + \\gamma E_{a'\\in\\pi(s')}[Q(s',a')|a,s,\\pi]]$\n",
        "\n",
        "For optimal policy, the above equation can be written as follows\n",
        "\n",
        "$Q^*(s,a) = r + \\gamma [Q^*(s',a')|a,s,\\pi]$ --> it is the most useful eqution in computing the $Q$ function using Deep neural networks\n",
        "\n",
        "### Advantage function\n",
        "$A^{\\pi}(s,a) = Q^\\pi(s,a) - V^\\pi(s,a)$ --> measures the advantage of taking an action $a$ in a state $s$. i.e., the relative measure of the importance of each action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "THiDa2lpeRNC"
      },
      "outputs": [],
      "source": [
        "class DuelingNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        super(DuelingNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.ll_fc1 = nn.Linear(state_size, 128) #lower layer fully connected for higher level features\n",
        "        self.ll_fc2 = nn.Linear(128, 128) #lower layer fully connected for higher level features\n",
        "        self.sl_fc3 = nn.Linear(128, 256) #stream layer fully connected\n",
        "        self.sl_fc_v = nn.Linear(256, 1) #stream layer fully connected for value\n",
        "        self.sl_fc_a = nn.Linear(256, action_size) #stream layer fully connected for advantage\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.ll_fc1(state))\n",
        "        x = F.relu(self.ll_fc2(x))\n",
        "        x = F.relu(self.sl_fc3(x))\n",
        "        self.stream_v = self.sl_fc_v(x)\n",
        "        self.stream_a = self.sl_fc_a(x)\n",
        "        q_values = self.stream_v + (self.stream_a - torch.mean(self.stream_a,1,True))\n",
        "        return q_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycyE88J0nmuo"
      },
      "source": [
        "## Define RL Agent\n",
        "\n",
        "### ϵ - greedy policy\n",
        "- Always sampling from memory is not a wise choice, as the agent will not explore other potential actions and can reach a sub optimal solution. For this reason using ϵ-greedy algorithm helps in avoiding such problem. In this, experiences are chosen with 1-ϵ probability and ranomdly with probability ϵ.\n",
        "\n",
        "- Begin with a higher ϵ value and then gradually decrease the epsilon to minimum value to provide the balance between exploration and efficiency.\n",
        "\n",
        "### soft updates to target network\n",
        "- An other modification to online Q-learning aimed at further improving the stability of our method with neural networks is to use a separate network for gen- erating the targets yj in the Q-learning update. More precisely, every C updates we clone the network Q to obtain a target network Q^ and use Q^ for generating the Q-learning targets yj for the following C updates to Q. This modification makes the algorithm more stable compared to standard online Q-learning, where an update that increases Q(st,at) often also increases Q(st 1 1,a) for all a and hence also increases the target yj, possibly leading to oscillations or divergence of the policy. Generating the targets using an older set of parameters adds a delay between the time an update to Q is made and the time the update affects the targets yj, making divergence or oscillations much more unlikely.\n",
        "\n",
        "### Double Q network\n",
        "Using same network ($\\theta^-$) for selecting an action and evaluating an action will over estimate the action-values (Q values). To reduce the impact of over estimation, it is useful to detach the selection and evaluation of the action. To this end, to select an action, use the same parameters ($\\theta$) as the actual network. To evaluate the action, we use the same approach as in DQN, i.e., update $\\theta^-$ periodically with $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Niq5v3GreSAg"
      },
      "outputs": [],
      "source": [
        "class DDQNAgent():\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = DuelingNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = DuelingNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=5e-4)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(int(1e5), 64)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > 64:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, 0.99)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        #Get the action that maximizes the Q values (argmax(Q)) --> uses local model not the target model (this is the difference bw DQN and DDQN)\n",
        "        # Get max predicted Q values (for next states) from target model\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1,actions)\n",
        "        # Compute Q targets for current states\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        # Get expected Q values from local model\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, 1e-3)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKiiSLi6Y3dc"
      },
      "source": [
        "\n",
        "* `self.qnetwork_target(next_states)`: This part of the code passes the batch of next states through the target Q-network to compute the Q-values for each action in each next state. The output is a tensor of shape `(batch_size, action_size)`.\n",
        "* `.detach()`: This method is used to detach the output tensor from the computation graph. This is done to prevent gradients from flowing back through the target Q-network during the optimization step.\n",
        "* `.max(1)[0]`: This part of the code computes the maximum Q-value for each next state in the batch. The `max` function returns a tuple of the maximum values and their indices, but we only care about the maximum values, so we take the first element of the tuple using `[0]`. The output is a tensor of shape `(batch_size,)`.\n",
        "* `.unsqueeze(1)`: This method is used to add a new dimension to the tensor at the specified position. In this case, we use `unsqueeze(1)` to add a new dimension to the tensor at position 1, which changes the shape of the tensor from `(batch_size,)` to `(batch_size, 1)`. This is done to make the tensor compatible with the shape of the other tensors in the computation graph.\n",
        "\n",
        "Overall, the line of code computes the maximum Q-value for each next state in the batch of experiences, detaches the output tensor from the computation graph to prevent gradients from flowing back through the target Q-network, and reshapes the tensor to have the same shape as the other tensors in the computation graph. This tensor is then used to compute the target Q-values for the current states in the batch of experiences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzH_5rHonugS"
      },
      "source": [
        "# Train and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axOtzju8ebzd",
        "outputId": "17a0b158-ecc1-49b2-e6e8-03a81793aa60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/naresh/anaconda3/envs/aiml_torch_gpu/lib/python3.12/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/naresh/anaconda3/envs/aiml_torch_gpu/lib/python3.12/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/naresh/anaconda3/envs/aiml_torch_gpu/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 19.08\n",
            "Episode 200\tAverage Score: 13.14\n",
            "Episode 300\tAverage Score: 11.65\n",
            "Episode 400\tAverage Score: 10.56\n",
            "Episode 500\tAverage Score: 10.00\n",
            "Episode 600\tAverage Score: 9.53\n",
            "Episode 700\tAverage Score: 9.71\n",
            "Episode 800\tAverage Score: 9.50\n",
            "Episode 900\tAverage Score: 9.49\n",
            "Episode 1000\tAverage Score: 37.33\n",
            "Episode 1100\tAverage Score: 228.42\n",
            "Episode 1200\tAverage Score: 301.37\n",
            "Episode 1300\tAverage Score: 210.41\n",
            "Episode 1400\tAverage Score: 393.25\n",
            "Episode 1500\tAverage Score: 254.67\n",
            "Episode 1600\tAverage Score: 169.39\n",
            "Episode 1700\tAverage Score: 140.86\n",
            "Episode 1800\tAverage Score: 125.60\n",
            "Episode 1900\tAverage Score: 125.13\n",
            "Episode 2000\tAverage Score: 137.36\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "def train_ddqn(agent, env, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon\n",
        "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "    \"\"\"\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        # if np.mean(scores_window)>=230.0:\n",
        "        #     print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "        #     torch.save(agent.qnetwork_local.state_dict(), './models/cartpole_d3qn.pth') #save model\n",
        "        #     break\n",
        "    return scores\n",
        "\n",
        "# Initialize the environment and the agent\n",
        "env_name = \"CartPole-v1\" #MountainCar-v0\n",
        "env = gym.make(env_name)\n",
        "agent = DDQNAgent(state_size=env.env.observation_space.shape[0], action_size=env.action_space.n, seed=0)\n",
        "\n",
        "# Train the agent\n",
        "scores = train_ddqn(agent, env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "np.savetxt('rewards_D3QN_cartpole', scores, delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "PwlMCEjghufV",
        "outputId": "63779d79-f6e5-4d83-b143-5456bc6891f3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "206.7601761817932\n"
          ]
        }
      ],
      "source": [
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.savefig('D3QN_cartpole.pdf', dpi=100, format='pdf')\n",
        "plt.show()\n",
        "end_time = time.time()\n",
        "execution_time = end_time-start_time\n",
        "print(execution_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model and test the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_agent(agent, env, episodes):\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=10)\n",
        "    for episode in range(episodes):\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "        time_steps = 0\n",
        "        while True:\n",
        "            action = agent.act(state)\n",
        "            next_state,reward,done,_ = env.step(action)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "            time_steps += 1\n",
        "        scores.append(score)\n",
        "        scores_window.append(score)\n",
        "        print('\\rEpisode:{}, \\tScore:{:.2f}'.format(episode, score))\n",
        "        # print('\\rNumber of steps:{}'.format(time_steps))\n",
        "    # if episode % 10 == 0:\n",
        "    #     print('\\rEpisode:{},\\tAverage Score: {:.2f}'.format(np.mean(scores_window)))\n",
        "    # if np.mean(scores_window) >= 200:\n",
        "        # print('\\nEnvironment solved in {:d} episodes, \\tAverage score:{:.2f}'.format(episode, np.mean(scores_window)))\n",
        "    print('\\rAverage score over {:d} episodes is:{:.2f}'.format(episodes, np.mean(scores_window)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:0, \tScore:500.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_23650/4078715864.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  agent.qnetwork_local.load_state_dict(torch.load('./models/cartpole_d3qn.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:1, \tScore:294.00\n",
            "Episode:2, \tScore:500.00\n",
            "Episode:3, \tScore:327.00\n",
            "Episode:4, \tScore:500.00\n",
            "Episode:5, \tScore:500.00\n",
            "Episode:6, \tScore:500.00\n",
            "Episode:7, \tScore:500.00\n",
            "Episode:8, \tScore:477.00\n",
            "Episode:9, \tScore:500.00\n",
            "Episode:10, \tScore:500.00\n",
            "Episode:11, \tScore:500.00\n",
            "Episode:12, \tScore:500.00\n",
            "Episode:13, \tScore:494.00\n",
            "Episode:14, \tScore:500.00\n",
            "Episode:15, \tScore:500.00\n",
            "Episode:16, \tScore:500.00\n",
            "Episode:17, \tScore:500.00\n",
            "Episode:18, \tScore:500.00\n",
            "Episode:19, \tScore:295.00\n",
            "Episode:20, \tScore:500.00\n",
            "Episode:21, \tScore:500.00\n",
            "Episode:22, \tScore:500.00\n",
            "Episode:23, \tScore:314.00\n",
            "Episode:24, \tScore:284.00\n",
            "Episode:25, \tScore:500.00\n",
            "Episode:26, \tScore:500.00\n",
            "Episode:27, \tScore:500.00\n",
            "Episode:28, \tScore:500.00\n",
            "Episode:29, \tScore:314.00\n",
            "Episode:30, \tScore:295.00\n",
            "Episode:31, \tScore:500.00\n",
            "Episode:32, \tScore:381.00\n",
            "Episode:33, \tScore:500.00\n",
            "Episode:34, \tScore:500.00\n",
            "Episode:35, \tScore:367.00\n",
            "Episode:36, \tScore:441.00\n",
            "Episode:37, \tScore:500.00\n",
            "Episode:38, \tScore:500.00\n",
            "Episode:39, \tScore:322.00\n",
            "Episode:40, \tScore:500.00\n",
            "Episode:41, \tScore:283.00\n",
            "Episode:42, \tScore:267.00\n",
            "Episode:43, \tScore:500.00\n",
            "Episode:44, \tScore:497.00\n",
            "Episode:45, \tScore:451.00\n",
            "Episode:46, \tScore:500.00\n",
            "Episode:47, \tScore:313.00\n",
            "Episode:48, \tScore:500.00\n",
            "Episode:49, \tScore:500.00\n",
            "Episode:50, \tScore:500.00\n",
            "Episode:51, \tScore:500.00\n",
            "Episode:52, \tScore:347.00\n",
            "Episode:53, \tScore:500.00\n",
            "Episode:54, \tScore:430.00\n",
            "Episode:55, \tScore:414.00\n",
            "Episode:56, \tScore:324.00\n",
            "Episode:57, \tScore:358.00\n",
            "Episode:58, \tScore:500.00\n",
            "Episode:59, \tScore:339.00\n",
            "Episode:60, \tScore:315.00\n",
            "Episode:61, \tScore:500.00\n",
            "Episode:62, \tScore:500.00\n",
            "Episode:63, \tScore:500.00\n",
            "Episode:64, \tScore:500.00\n",
            "Episode:65, \tScore:500.00\n",
            "Episode:66, \tScore:376.00\n",
            "Episode:67, \tScore:460.00\n",
            "Episode:68, \tScore:500.00\n",
            "Episode:69, \tScore:500.00\n",
            "Episode:70, \tScore:500.00\n",
            "Episode:71, \tScore:500.00\n",
            "Episode:72, \tScore:500.00\n",
            "Episode:73, \tScore:288.00\n",
            "Episode:74, \tScore:385.00\n",
            "Episode:75, \tScore:500.00\n",
            "Episode:76, \tScore:500.00\n",
            "Episode:77, \tScore:500.00\n",
            "Episode:78, \tScore:500.00\n",
            "Episode:79, \tScore:500.00\n",
            "Episode:80, \tScore:500.00\n",
            "Episode:81, \tScore:381.00\n",
            "Episode:82, \tScore:317.00\n",
            "Episode:83, \tScore:342.00\n",
            "Episode:84, \tScore:500.00\n",
            "Episode:85, \tScore:341.00\n",
            "Episode:86, \tScore:500.00\n",
            "Episode:87, \tScore:311.00\n",
            "Episode:88, \tScore:292.00\n",
            "Episode:89, \tScore:265.00\n",
            "Episode:90, \tScore:500.00\n",
            "Episode:91, \tScore:326.00\n",
            "Episode:92, \tScore:500.00\n",
            "Episode:93, \tScore:500.00\n",
            "Episode:94, \tScore:500.00\n",
            "Episode:95, \tScore:500.00\n",
            "Episode:96, \tScore:500.00\n",
            "Episode:97, \tScore:336.00\n",
            "Episode:98, \tScore:500.00\n",
            "Episode:99, \tScore:500.00\n",
            "Average score over 100 episodes is:466.20\n"
          ]
        }
      ],
      "source": [
        "# initialize the environment\n",
        "# env = gym.make('MountainCar-v0') #CartPole-v1\n",
        "# agent = DDQNAgent(state_size=env.env.observation_space.shape[0], action_size=env.action_space.n, seed=0)\n",
        "# load the weights\n",
        "agent.qnetwork_local.load_state_dict(torch.load('./models/cartpole_d3qn.pth'))\n",
        "# test the agent\n",
        "test_agent(agent, env, episodes = 100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
